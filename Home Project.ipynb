{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2528265-14f9-491d-9d25-80e0ffef3f2f",
   "metadata": {},
   "source": [
    "## 1. Import data and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c76839b-cfc7-4e39-858f-dafc61ea9d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4075c1b6-ecb4-4633-9689-eed90937021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from the pkl file \n",
    "objects = []\n",
    "with (open(\"home_project.pkl\", \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            objects.append(pickle.load(openfile))\n",
    "        except EOFError:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0211ef36-1416-4b6d-9043-7db238b8d39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign names to the dataframes\n",
    "project_tf_idf_mat = objects[0]['project_tf_idf_mat']\n",
    "project_df =  objects[0]['project_df']\n",
    "holdout_tf_idf_mat =  objects[0]['holdout_tf_idf_mat']\n",
    "holdout_df = objects[0]['holdout_df']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c4e163-186f-405c-996e-234c317f8099",
   "metadata": {},
   "source": [
    "## 2. Inspect and preprocess the data\n",
    "- text_features is a dictionary, it needs to be split to columns of feature\n",
    "- country has some missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11196283-a052-484f-844a-f83c8674aa93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint_id</th>\n",
       "      <th>invoice_arrival_date</th>\n",
       "      <th>country</th>\n",
       "      <th>rel_doc</th>\n",
       "      <th>text_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14240</td>\n",
       "      <td>2022-01-03 18:09:53.421000+00:00</td>\n",
       "      <td>AU</td>\n",
       "      <td>True</td>\n",
       "      <td>{'num_of_rows': 98, 'num_of_punc_in_text_words...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35837</td>\n",
       "      <td>2021-01-18 13:07:49.108000+00:00</td>\n",
       "      <td>AU</td>\n",
       "      <td>True</td>\n",
       "      <td>{'num_of_rows': 19, 'num_of_punc_in_text_words...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32165</td>\n",
       "      <td>2021-11-05 00:06:48.725000+00:00</td>\n",
       "      <td>AU</td>\n",
       "      <td>True</td>\n",
       "      <td>{'num_of_rows': 47, 'num_of_punc_in_text_words...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56670</td>\n",
       "      <td>2021-04-05 19:08:41.746000+00:00</td>\n",
       "      <td>AU</td>\n",
       "      <td>True</td>\n",
       "      <td>{'num_of_rows': 9, 'num_of_punc_in_text_words'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38372</td>\n",
       "      <td>2021-02-02 13:39:24.751000+00:00</td>\n",
       "      <td>AU</td>\n",
       "      <td>True</td>\n",
       "      <td>{'num_of_rows': 76, 'num_of_punc_in_text_words...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   datapoint_id             invoice_arrival_date country  rel_doc  \\\n",
       "0         14240 2022-01-03 18:09:53.421000+00:00      AU     True   \n",
       "1         35837 2021-01-18 13:07:49.108000+00:00      AU     True   \n",
       "2         32165 2021-11-05 00:06:48.725000+00:00      AU     True   \n",
       "3         56670 2021-04-05 19:08:41.746000+00:00      AU     True   \n",
       "4         38372 2021-02-02 13:39:24.751000+00:00      AU     True   \n",
       "\n",
       "                                       text_features  \n",
       "0  {'num_of_rows': 98, 'num_of_punc_in_text_words...  \n",
       "1  {'num_of_rows': 19, 'num_of_punc_in_text_words...  \n",
       "2  {'num_of_rows': 47, 'num_of_punc_in_text_words...  \n",
       "3  {'num_of_rows': 9, 'num_of_punc_in_text_words'...  \n",
       "4  {'num_of_rows': 76, 'num_of_punc_in_text_words...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the data \n",
    "project_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baa07fd-9ee6-43dd-a107-7e6c1dc60e69",
   "metadata": {},
   "source": [
    "The features are in a dictionary form, they have to be split to columns in order to understand what is going on here. The dictionaries are all strings so they have to be converted to dictionaries and then they can be converted to Pandas Series and used as columns of data. The function literal_eval will convert the text to dictionaries and the function pd.Series will convert the dictionaries to columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b54e3-8a33-4542-b5c7-967cb8d982b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply literal_eval to the text column\n",
    "project_df.text_features.apply(literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0389af89-0b71-48d7-ac24-ad0ee80b006c",
   "metadata": {},
   "source": [
    "It not working, let's check if we can locate problematic strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d59e37e-d84d-4fa3-8f80-aa011fe6b4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22645\n",
      "23209\n"
     ]
    }
   ],
   "source": [
    "# iterate over all strings and try to find where literal_eval is not working \n",
    "for i in range(len(project_df.text_features)):\n",
    "    try:\n",
    "        literal_eval(project_df.text_features[i])\n",
    "    except Exception:\n",
    "        print(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a221c-9872-47cd-b306-ab75671cfc94",
   "metadata": {},
   "source": [
    "only two strings are causing the problems, let's take a closer look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "990c3294-7436-4d8f-8564-137526948976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'num_of_rows': 1, 'num_of_punc_in_text_words': 0, 'num_of_punc_in_text_chars': 0, 'lines_made_of_symbols': 0, 'empty _spaces': 1, 'characters_in_raw_invoice': 2, 'words_raw_invoice_by_split': 0, 'ascii_characters_in_invoice': 0, 'words_capital_first': 0, 'words_all_uppercase': 0, 'alphanumeric_words': 0, 'words_repeated_characters': 0, 'web_adresses': 0, 'email_adresses': 0, 'num_of_digits': 0, 'solo_numbers': 0, 'float_point_numbers': 0, 'numbers_line_delimited': 0, 'total_number_of_numbers_in_invoice': 0, 'punc_prop': nan, 'lines_symbols_prop': nan, 'num_of_chrs_prop': inf, 'words_num_prop': nan, 'ascii_characters_to_prop': nan, 'words_capital_first_prop': nan, 'words_all_uppercase_prop': nan, 'alphanumeric_words_prop': nan, 'words_repeated_characters_prop': nan, 'digits_in_invoice_prop': nan, 'solo_numbers_prop': nan, 'float_point_numbers_prop': nan}\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_df.text_features[22645]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f642003-b5b2-45d2-96bd-f48a54ccb170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'num_of_rows': 1, 'num_of_punc_in_text_words': 0, 'num_of_punc_in_text_chars': 0, 'lines_made_of_symbols': 0, 'empty _spaces': 0, 'characters_in_raw_invoice': 1, 'words_raw_invoice_by_split': 0, 'ascii_characters_in_invoice': 0, 'words_capital_first': 0, 'words_all_uppercase': 0, 'alphanumeric_words': 0, 'words_repeated_characters': 0, 'web_adresses': 0, 'email_adresses': 0, 'num_of_digits': 0, 'solo_numbers': 0, 'float_point_numbers': 0, 'numbers_line_delimited': 0, 'total_number_of_numbers_in_invoice': 0, 'punc_prop': nan, 'lines_symbols_prop': nan, 'num_of_chrs_prop': inf, 'words_num_prop': nan, 'ascii_characters_to_prop': nan, 'words_capital_first_prop': nan, 'words_all_uppercase_prop': nan, 'alphanumeric_words_prop': nan, 'words_repeated_characters_prop': nan, 'digits_in_invoice_prop': nan, 'solo_numbers_prop': nan, 'float_point_numbers_prop': nan}\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_df.text_features[23209]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a863528-8b8b-4d28-a903-e9f9d84c31be",
   "metadata": {},
   "source": [
    "Both of these look like empty documents. I think it's safe to just remove these rows from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ef14f9e-ce93-4682-9d54-096cc3e582f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the bad rows \n",
    "project_df.drop([22645, 23209], axis= 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58f61dff-c12f-4b23-8c8e-750edc73f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform text_features to columns of data and save the new dataframe.\n",
    "project_df = pd.concat([project_df.drop(['text_features'], axis =1), project_df.text_features.apply(literal_eval).apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aee0505-ce0e-4be9-b7c4-506d8bb94582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 49998 entries, 0 to 49999\n",
      "Data columns (total 35 columns):\n",
      " #   Column                              Non-Null Count  Dtype              \n",
      "---  ------                              --------------  -----              \n",
      " 0   datapoint_id                        49998 non-null  int64              \n",
      " 1   invoice_arrival_date                49998 non-null  datetime64[ns, UTC]\n",
      " 2   country                             48155 non-null  object             \n",
      " 3   rel_doc                             49998 non-null  bool               \n",
      " 4   num_of_rows                         49998 non-null  float64            \n",
      " 5   num_of_punc_in_text_words           49998 non-null  float64            \n",
      " 6   num_of_punc_in_text_chars           49998 non-null  float64            \n",
      " 7   lines_made_of_symbols               49998 non-null  float64            \n",
      " 8   empty _spaces                       49998 non-null  float64            \n",
      " 9   characters_in_raw_invoice           49998 non-null  float64            \n",
      " 10  words_raw_invoice_by_split          49998 non-null  float64            \n",
      " 11  ascii_characters_in_invoice         49998 non-null  float64            \n",
      " 12  words_capital_first                 49998 non-null  float64            \n",
      " 13  words_all_uppercase                 49998 non-null  float64            \n",
      " 14  alphanumeric_words                  49998 non-null  float64            \n",
      " 15  words_repeated_characters           49998 non-null  float64            \n",
      " 16  web_adresses                        49998 non-null  float64            \n",
      " 17  email_adresses                      49998 non-null  float64            \n",
      " 18  num_of_digits                       49998 non-null  float64            \n",
      " 19  solo_numbers                        49998 non-null  float64            \n",
      " 20  float_point_numbers                 49998 non-null  float64            \n",
      " 21  numbers_line_delimited              49998 non-null  float64            \n",
      " 22  total_number_of_numbers_in_invoice  49998 non-null  float64            \n",
      " 23  punc_prop                           49998 non-null  float64            \n",
      " 24  lines_symbols_prop                  49998 non-null  float64            \n",
      " 25  num_of_chrs_prop                    49998 non-null  float64            \n",
      " 26  words_num_prop                      49998 non-null  float64            \n",
      " 27  ascii_characters_to_prop            49998 non-null  float64            \n",
      " 28  words_capital_first_prop            49998 non-null  float64            \n",
      " 29  words_all_uppercase_prop            49998 non-null  float64            \n",
      " 30  alphanumeric_words_prop             49998 non-null  float64            \n",
      " 31  words_repeated_characters_prop      49998 non-null  float64            \n",
      " 32  digits_in_invoice_prop              49998 non-null  float64            \n",
      " 33  solo_numbers_prop                   49998 non-null  float64            \n",
      " 34  float_point_numbers_prop            49998 non-null  float64            \n",
      "dtypes: bool(1), datetime64[ns, UTC](1), float64(31), int64(1), object(1)\n",
      "memory usage: 13.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# take a look at the data, look for missing values\n",
    "project_df.info()\n",
    "\n",
    "# only country has missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c8174-2bf5-4811-8a11-067e2c068367",
   "metadata": {},
   "source": [
    "The only variable with missing values is 'country', I will replace the missing values with 'missing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "305df0ab-dd9e-4f2d-86f5-665010d7a983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing countries with 'missing'\n",
    "project_df['country'].fillna('missing', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f81fcb46-db97-4cb9-8a91-883ce9f28059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the data to new dataframe so that we can always go back\n",
    "from copy import deepcopy\n",
    "df_copy = deepcopy(project_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0f89013f-db54-42b4-a06f-0be714ff96de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace countries with small number of counts with 'other'\n",
    "country_count = df_copy.country.value_counts()\n",
    "under_50 = country_count[country_count<50]\n",
    "df_copy.loc[df_copy[\"country\"].isin(under_50.index.tolist()), 'country'] = \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7c73ab9-f9a6-4e18-97e6-e93f83d8e331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU         39670\n",
       "DK          5321\n",
       "missing     1843\n",
       "DE           731\n",
       "US           689\n",
       "other        335\n",
       "GB           238\n",
       "FR           232\n",
       "PL           221\n",
       "HU           133\n",
       "NZ           118\n",
       "AT           101\n",
       "NL            97\n",
       "LU            59\n",
       "IT            58\n",
       "CH            51\n",
       "NO            51\n",
       "ES            50\n",
       "Name: country, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.country.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f81c43-a691-4559-b609-ad5483971cbd",
   "metadata": {},
   "source": [
    "I need to remove the two problematic rows from the sparse csr matrix as well. I found the following function online, I didn't implement it myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ace50d8-1528-4068-b233-124fced0a511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_rows_csr(mat, indices):\n",
    "    \"\"\"\n",
    "    Remove the rows denoted by ``indices`` form the CSR sparse matrix ``mat``.\n",
    "    \"\"\"\n",
    "    if not isinstance(mat, scipy.sparse.csr_matrix):\n",
    "        raise ValueError(\"works only for CSR format -- use .tocsr() first\")\n",
    "    indices = list(indices)\n",
    "    mask = np.ones(mat.shape[0], dtype=bool)\n",
    "    mask[indices] = False\n",
    "    return mat[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9872371d-06ed-427a-8673-1c643a3405ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_tf_idf_mat = delete_rows_csr(project_tf_idf_mat, [22645, 23209])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d4636d-3a60-4eec-a237-b877d1fed978",
   "metadata": {},
   "source": [
    "Now I want to get rid of the id and date columns, I don't think they will be helpful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3b9714e-cc62-4ed9-ad3a-38c86209fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.drop(['datapoint_id', 'invoice_arrival_date'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9123353-7f32-4a26-8d26-10d2e4606983",
   "metadata": {},
   "source": [
    "Now convert country to dummies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "753974d4-57a2-4e65-bfe7-bb7d24737c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = pd.get_dummies(df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0331fa09-0c6f-4f78-8f01-0923215ca30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rel_doc</th>\n",
       "      <th>num_of_rows</th>\n",
       "      <th>num_of_punc_in_text_words</th>\n",
       "      <th>num_of_punc_in_text_chars</th>\n",
       "      <th>lines_made_of_symbols</th>\n",
       "      <th>empty _spaces</th>\n",
       "      <th>characters_in_raw_invoice</th>\n",
       "      <th>words_raw_invoice_by_split</th>\n",
       "      <th>ascii_characters_in_invoice</th>\n",
       "      <th>words_capital_first</th>\n",
       "      <th>...</th>\n",
       "      <th>country_HU</th>\n",
       "      <th>country_IT</th>\n",
       "      <th>country_LU</th>\n",
       "      <th>country_NL</th>\n",
       "      <th>country_NO</th>\n",
       "      <th>country_NZ</th>\n",
       "      <th>country_PL</th>\n",
       "      <th>country_US</th>\n",
       "      <th>country_missing</th>\n",
       "      <th>country_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>2407.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1954.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>1187.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>951.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>937.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>749.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rel_doc  num_of_rows  num_of_punc_in_text_words  num_of_punc_in_text_chars  \\\n",
       "0     True         98.0                        0.0                        0.0   \n",
       "1     True         19.0                        0.0                        0.0   \n",
       "2     True         47.0                        0.0                        0.0   \n",
       "3     True          9.0                        0.0                        0.0   \n",
       "4     True         76.0                        0.0                        0.0   \n",
       "\n",
       "   lines_made_of_symbols  empty _spaces  characters_in_raw_invoice  \\\n",
       "0                    0.0          349.0                     2407.0   \n",
       "1                    0.0           16.0                      172.0   \n",
       "2                    0.0          184.0                     1187.0   \n",
       "3                    0.0           20.0                      119.0   \n",
       "4                    0.0          107.0                      937.0   \n",
       "\n",
       "   words_raw_invoice_by_split  ascii_characters_in_invoice  \\\n",
       "0                       400.0                       1954.0   \n",
       "1                        30.0                        131.0   \n",
       "2                       196.0                        951.0   \n",
       "3                        21.0                         88.0   \n",
       "4                       133.0                        749.0   \n",
       "\n",
       "   words_capital_first  ...  country_HU  country_IT  country_LU  country_NL  \\\n",
       "0                  0.0  ...           0           0           0           0   \n",
       "1                  0.0  ...           0           0           0           0   \n",
       "2                  0.0  ...           0           0           0           0   \n",
       "3                  0.0  ...           0           0           0           0   \n",
       "4                  0.0  ...           0           0           0           0   \n",
       "\n",
       "   country_NO  country_NZ  country_PL  country_US  country_missing  \\\n",
       "0           0           0           0           0                0   \n",
       "1           0           0           0           0                0   \n",
       "2           0           0           0           0                0   \n",
       "3           0           0           0           0                0   \n",
       "4           0           0           0           0                0   \n",
       "\n",
       "   country_other  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bf347b-3a89-4b85-9176-b19e73e9397e",
   "metadata": {},
   "source": [
    "We have prepared the data, time to model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fc408b-5048-4d8f-89f3-205222662c5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. build models\n",
    " - Feature and target values: X,y\n",
    " - combine training data with tf-idf matrix\n",
    " - Train test split\n",
    " - train a few algorithms\n",
    " - Deal with imbalanced classes\n",
    " - I will train two models: Random Forest and Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69688ed7-5795-44e4-be35-07c1692fe3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d09f73-1663-4996-b687-f272d1b8e3a7",
   "metadata": {},
   "source": [
    "### Reduce the dimension of the tf-idf matrix using truncated SVD\n",
    " The tf-idf matrix is way too big to work with, I will combine it with the text features and reduce the dimension using truncated SVD. This is common for sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a4c622f-1a77-468f-8793-02c034e8a017",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncatedSVD = TruncatedSVD(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56ca0c38-16db-4957-b8d9-31e33572b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to features and target data and labels\n",
    "y = df_copy['rel_doc']\n",
    "X = df_copy.drop(['rel_doc'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf7bc6c-da93-4cb2-be20-9618c8a6b7da",
   "metadata": {},
   "source": [
    "combine the dataframe with the tf-idf matrix and save the whole thing as a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f0ee05b-36fa-4364-a7b0-9c6ff6cb1923",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sparse.hstack([project_tf_idf_mat, X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb40926a-a27d-4ca7-b8eb-f4b56a621598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split to train and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.3,  random_state = 1, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2c840e6-dc1e-4f80-b0f5-37fb0f2267f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(n_components=150)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncatedSVD.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "695dea40-13b1-4b1f-83a2-43aed9acdf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_X_train = truncatedSVD.transform(X_train)\n",
    "truncated_X_test = truncatedSVD.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d525cfb9-5828-4bb8-8189-7d5a8b37dfdf",
   "metadata": {},
   "source": [
    "Check for imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4cc6e57-46f0-4a81-9d60-f99fa36f429e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     0.928897\n",
       "False    0.071103\n",
       "Name: rel_doc, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True) \n",
    "# pretty imbablanced... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb09b502-b0b5-408a-ad26-e790ea1fda36",
   "metadata": {},
   "source": [
    "### Setup ML pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5dbc625c-88ea-4986-bcd9-b924f08add16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbbf9a2-23d8-4de1-a313-da3d41c6eaa5",
   "metadata": {},
   "source": [
    "I will train a Random Forest classifier and a Gradient Boosting classsifier. Obviously, many more classifiers can be considered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c407915-6e05-4897-a99f-920f8c5fde64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup up pipelines to stack scaling and modelling \n",
    "pipelines = {\n",
    "    'rf': make_pipeline(StandardScaler(), RandomForestClassifier(random_state=1)),\n",
    "    'gb': make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=1))\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2805308-3cb1-43f7-b301-73fd40dbd7fd",
   "metadata": {},
   "source": [
    "I will use grid search to optimize the n_estimators parameter for both models. Obviously, this can be done for all the parameters of the models, depending on how much time we want to invest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8458553f-ce8b-4a3a-8bc6-d3a26bae7598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search for tuning parameters\n",
    "grid = {\n",
    "     'rf': {\n",
    "         'randomforestclassifier__n_estimators' : [100, 200, 300] \n",
    "     },\n",
    "     'gb':{\n",
    "         'gradientboostingclassifier__n_estimators' : [100, 200, 300] \n",
    "     }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1ad05ae9-0b41-496a-b104-c107fad9a626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training the rf model\n",
      "training the gb model\n"
     ]
    }
   ],
   "source": [
    "# find the best hyperparameters using GridSearchCV and save the best model\n",
    "\n",
    "# create a blank dictionary to hold models\n",
    "fit_models = {}\n",
    "# loop over algorithms and choose hyperparameters using GrisSearchCV\n",
    "for algo, pipeline in pipelines.items():\n",
    "    print(f'training the {algo} model')\n",
    "    model = GridSearchCV(pipeline, grid[algo], n_jobs = -1, cv=10)\n",
    "    model.fit(truncated_X_train, y_train)\n",
    "    fit_models[algo] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765c8fe6-3542-42de-b9c5-eb52577a0140",
   "metadata": {},
   "source": [
    "## 4. Evaluate performance on test partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cb015ace-ed29-4094-b362-0454939414a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f7d1e79-224a-4935-b437-0c0cb452900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the test dataset\n",
    "truncated_X_test = truncatedSVD.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f9a231-1b43-47ec-bd07-3900c7f743a5",
   "metadata": {},
   "source": [
    "For shortness of time I will consider a threshold of 0.5 for both models, This is not ideal since the data is very imbalanced. With more time I would try to optimize this threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "48e51e1a-20d9-4103-a2b4-41d716efdff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Confusion matrix for rf:\n",
      "[[  572   495]\n",
      " [  125 13808]]\n",
      " Confusion matrix for gb:\n",
      "[[  601   466]\n",
      " [  170 13763]]\n"
     ]
    }
   ],
   "source": [
    "# look at confusion matrices for both models\n",
    "\n",
    "for algo, model in fit_models.items():\n",
    "    yhat = model.predict(truncated_X_test)\n",
    "    cm = confusion_matrix(y_test, yhat)\n",
    "    print(f' Confusion matrix for {algo}:')\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5132d84c-b5af-4f24-8ba9-cbe7d2e45d2b",
   "metadata": {},
   "source": [
    "Both models are performing very well on the majority class, the performance on the minority class is not bad but it can probably be improved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3573df19-8d1d-42ab-8554-a3e8e93bb7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics for rf: accuracy = 0.959, precision = 0.965, recall = 0.991\n",
      "metrics for gb: accuracy = 0.958, precision = 0.967, recall = 0.988\n"
     ]
    }
   ],
   "source": [
    "# evaluate the performance of the models\n",
    "\n",
    "for algo, model in fit_models.items():\n",
    "    yhat = model.predict(truncated_X_test)\n",
    "    accuracy = np.round(accuracy_score(y_test, yhat),3)\n",
    "    precision = np.round(precision_score(y_test, yhat),3)\n",
    "    recall = np.round(recall_score(y_test, yhat),3)\n",
    "    print(f'metrics for {algo}: accuracy = {accuracy}, precision = {precision}, recall = {recall}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bc30b9-d431-4ca9-852f-195e0815a6ed",
   "metadata": {},
   "source": [
    "### Save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "391334c9-9673-4fa6-9f40-beaa57496003",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('RandomForestModel.pkl', 'wb') as f:\n",
    "    pickle.dump(fit_models['rf'], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad5aaf4-81e0-44b9-a4d4-d65630697959",
   "metadata": {},
   "source": [
    "## 5. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f02efc21-6db9-4fd4-9e87-014207e5752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform text_features to columns of data and save the new dataframe.\n",
    "holdout_df = pd.concat([holdout_df.drop(['text_features'], axis =1), holdout_df.text_features.apply(literal_eval).apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bbf3440a-f45d-4141-ac96-ff9fa14b777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing countries with 'missing'\n",
    "holdout_df['country'].fillna('missing', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "722d9fb9-1bf3-4afb-ab67-70abf84105bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the data to new dataframe so that we can always go back\n",
    "from copy import deepcopy\n",
    "holdout_copy = deepcopy(holdout_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d72ed8b7-bab9-476a-a235-1e1ccffd5b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace countries with small number of counts with 'other'\n",
    "holdout_copy.loc[holdout_copy[\"country\"].isin(under_50.index.tolist()), 'country'] = \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "82f9f445-acf9-4340-9df4-878cf062dc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace countries with small number of counts with 'other', some countries that were not in the training data\n",
    "country_count = holdout_copy.country.value_counts()\n",
    "under_5 = country_count[country_count<5]\n",
    "holdout_copy.loc[holdout_copy[\"country\"].isin(under_5.index.tolist()), 'country'] = \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "88fcff75-6b11-4adc-ab39-dc9826eb94ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_copy.drop(['datapoint_id', 'invoice_arrival_date'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "02c7311a-cb65-4bd0-85bd-97e89078daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_copy = pd.get_dummies(holdout_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e37e2147-56d4-4f4f-b2d8-d7117c0bec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sparse.hstack([holdout_tf_idf_mat, holdout_copy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4c02111a-54a6-47e1-b875-321de0de7ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_data = truncatedSVD.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7095758b-4845-4c5d-aa59-45c5a2288336",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = fit_models['rf'].predict(truncated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3ccf29b9-6f59-40d2-b50e-bd5594b91b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_submission = pd.DataFrame([holdout_df.datapoint_id, prediction]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1fb55bab-450e-4902-9f27-6c12af0b0b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_submission.to_parquet('submission.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa55d3ec-dc1e-4b10-b084-da22144d3eaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## If I had more time\n",
    "- Better preprocessing for text features: I could look at the relationships bertween these variables, and maybe come up with some insights that can improve the modelling\n",
    "- More classifiers : I tried only two algorithms - Random Forest and Gradient Boosting, many more classifiers can be tried out, like Naive Bayes, Neural Network etc.\n",
    "- More hyperparameter tuning: I only tuned one hyperparameter in the models using cross validation over three values. Any one of the other hyperparameters can also be tuned and improved.\n",
    "- Find optimal number of components for truncated SVD: I used 150 components without any strong justification, the correct thing to do is to try out different numbers and look at the variance in the data for each one of these numbers. Then we can choose a value that doesn't lose too much information.\n",
    "- More attention to imbalance: The data is very imbalanced and I didn't really take this into account in the modelling, with more time I would consider resampling, class weights in the model fit and choosing a better threshold than 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (tags/v3.8.8:024d805, Feb 19 2021, 13:18:16) [MSC v.1928 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "14b42af619014e35325e49c45b3eb2852b785bc18d13b4dce70b076fe1a37f18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
